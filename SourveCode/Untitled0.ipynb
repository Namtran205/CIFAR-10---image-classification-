{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOQQvgK5X1HnR7eh/fvh0M9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K9SHphxMxbAi"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from torch.optim import SGD\n","# from cnn_network import CNN # Lớp CNN sẽ được định nghĩa trực tiếp tại đây\n","from tqdm import tqdm\n","from multiprocessing import freeze_support\n","\n","# --- MLP Model Definition (commented out as per request) ---\n","# class MLP(nn.Module):\n","#     def __init__(self):\n","#         super(MLP, self).__init__()\n","#         self.fc1 = nn.Linear(32*32*3, 1024)\n","#         self.bn1 = nn.BatchNorm1d(1024)\n","#         self.fc2 = nn.Linear(1024, 512)\n","#         self.bn2 = nn.BatchNorm1d(512)\n","#         self.fc3 = nn.Linear(512, 10)\n","#         self.dropout = nn.Dropout(0.5)\n","#         self.relu = nn.ReLU()\n","#     def forward(self, x):\n","#         x = x.view(x.size(0), -1)\n","#         x = self.fc1(x)\n","#         x = self.bn1(x)\n","#         x = self.relu(x)\n","#         x = self.dropout(x)\n","#         x = self.fc2(x)\n","#         x = self.bn2(x)\n","#         x = self.relu(x)\n","#         x = self.dropout(x)\n","#         x = self.fc3(x)\n","#         return x\n","\n","# --- CNN Model Definition ---\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","        self.dropout = nn.Dropout(0.2)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.pool(self.relu(self.conv1(x)))\n","        x = self.pool(self.relu(self.conv2(x)))\n","        x = self.pool(self.relu(self.conv3(x)))\n","        x = x.view(-1, 128 * 4 * 4)\n","        x = self.dropout(x)\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# Load CIFAR-10 dataset\n","train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Split train into train and validation\n","val_size = int(0.1 * len(train_set))\n","train_size = len(train_set) - val_size\n","train_set, val_set = torch.utils.data.random_split(train_set, [train_size, val_size])\n","\n","print(f\"Training set size: {len(train_set)}\")\n","print(f\"Validation set size: {len(val_set)}\")\n","print(f\"Test set size: {len(test_set)}\")\n","\n","# Data loaders\n","batch_size = 128 # Keep consistent with MLP if comparing\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","# Class names\n","classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# --- evaluate_model function (common for both) ---\n","def evaluate_model(model, loader, criterion):\n","    model.eval()\n","    loss_cnt = 0.0\n","    accuracy = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images, labels = images.to(device), labels.to(device)\n","            output = model(images)\n","            loss = criterion(output, labels)\n","\n","            loss_cnt += loss.item()\n","            _, predicted = torch.max(output.data, 1)\n","            total += labels.size(0)\n","            accuracy += (predicted == labels).sum().item()\n","\n","    loss = loss_cnt / len(loader)\n","    acc = 100 * accuracy / total\n","    return loss, acc\n","\n","# --- plot_confusion_matrix function (common for both) ---\n","def plot_confusion_matrix(all_labels, all_preds, classes, model_name=\"Model\"):\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","    plt.title(f'Confusion Matrix for {model_name}')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.savefig(f'{model_name}_confusion_matrix.png')\n","    plt.show()\n","\n","# --- train_model function (common for both) ---\n","def train_model(model, train_loader, val_loader, num_epochs=15, model_name='Model'):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","    # Data for custom learning curve plot\n","    train_cnt= []\n","    train_losses = []\n","    train_accs = [] # To store training accuracy for plot\n","\n","    val_cnt = [] # X-axis for validation, same granularity as train if interval is same\n","    val_losses= []\n","    val_accs = [] # To store validation accuracy for plot\n","\n","    # Frequency for recording train/val metrics\n","    record_metrics_every_batches = 10 # Record train loss/acc and evaluate val loss/acc every 10 batches\n","\n","    print(f\"\\n--- Starting Training for {model_name} ---\")\n","    for epoch in range(num_epochs): # epoch is 0-indexed here (0, 1, ..., num_epochs-1)\n","        model.train()\n","        running_loss_epoch = 0.0 # Accumulate loss for epoch summary\n","        correct_train_epoch = 0 # Accumulate correct predictions for epoch summary\n","        total_train_epoch = 0 # Accumulate total samples for epoch summary\n","\n","        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [{model_name}]')):\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","            loss_val = criterion(outputs, labels)\n","            optimizer.zero_grad()\n","            loss_val.backward()\n","            optimizer.step()\n","\n","            running_loss_epoch += loss_val.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_train_epoch += labels.size(0)\n","            correct_train_epoch += (predicted == labels).sum().item()\n","\n","            # --- Calculate current examples seen based on your logic, corrected for 0-indexed epoch ---\n","            current_trained_nums = (epoch * len(train_loader.dataset)) + (batch_idx * batch_size)\n","\n","            # --- Collect data for continuous Train Loss & Accuracy plot ---\n","            if batch_idx % record_metrics_every_batches == 0:\n","                train_losses.append(loss_val.item())\n","                train_cnt.append(current_trained_nums)\n","\n","                # Calculate current batch accuracy for plotting\n","                current_batch_acc = 100 * (predicted == labels).sum().item() / labels.size(0)\n","                train_accs.append(current_batch_acc)\n","\n","                # --- Evaluate Validation Loss & Accuracy PERIODICALLY for line plot ---\n","                val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n","                val_losses.append(val_loss)\n","                val_accs.append(val_acc)\n","                val_cnt.append(current_trained_nums) # Same counter as train\n","\n","        # Calculate and print epoch summary (using accumulated values)\n","        epoch_train_loss = running_loss_epoch / len(train_loader)\n","        epoch_train_acc = 100 * correct_train_epoch / total_train_epoch\n","\n","        # For print, use the last recorded periodic validation.\n","        final_epoch_val_loss = val_losses[-1] if val_losses else float('nan')\n","        final_epoch_val_acc = val_accs[-1] if val_accs else float('nan')\n","\n","        print(f'Epoch {epoch+1}: Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%, Val Loss: {final_epoch_val_loss:.4f}, Val Acc: {final_epoch_val_acc:.2f}%')\n","\n","    print(f\"--- Finished Training for {model_name} ---\")\n","\n","    # --- Plotting Learning Curves (Loss) ---\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_cnt, train_losses, color='blue', label='Training Loss')\n","    plt.plot(val_cnt, val_losses, color='red', label='Validation Loss')\n","    plt.legend(loc='upper right')\n","    plt.xlabel('Number of training examples seen')\n","    plt.ylabel('Loss (Cross-Entropy Loss)')\n","    plt.title(f'Learning Curve: {model_name} (Train vs Validation Loss)')\n","    plt.grid(True)\n","\n","    # --- Plotting Learning Curves (Accuracy) ---\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_cnt, train_accs, color='blue', label='Training Accuracy')\n","    plt.plot(val_cnt, val_accs, color='red', label='Validation Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.xlabel('Number of training examples seen')\n","    plt.ylabel('Accuracy (%)')\n","    plt.title(f'Learning Curve: {model_name} (Train vs Validation Accuracy)')\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{model_name}_learning_curves_loss_acc.png')\n","    plt.show()\n","\n","    return model\n","\n","# --- Main execution block for CNN ---\n","if __name__ == '__main__':\n","    freeze_support() # Essential for multiprocessing on Windows\n","\n","    cnn_model = CNN().to(device) # Initialize CNN model\n","\n","    num_epochs = 15 # Number of epochs for CNN training\n","\n","    # --- Train CNN and plot its custom learning curve ---\n","    print(\"\\nTraining CNN and plotting its custom learning curve...\")\n","    cnn_model = train_model(cnn_model, train_loader, val_loader, num_epochs=num_epochs, model_name='CNN')\n","\n","    # --- Final Evaluation on Test Set and Confusion Matrix Plotting for CNN ---\n","    final_criterion = nn.CrossEntropyLoss()\n","\n","    print(\"\\nFinal Evaluation for CNN on test set...\")\n","    all_cnn_preds_test = []\n","    all_cnn_labels_test = []\n","    cnn_model.eval()\n","    cnn_test_running_loss = 0.0\n","    cnn_test_correct = 0\n","    cnn_test_total = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = cnn_model(images)\n","            loss = final_criterion(outputs, labels)\n","            cnn_test_running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            cnn_test_correct += (predicted == labels).sum().item()\n","            cnn_test_total += labels.size(0)\n","            all_cnn_preds_test.extend(predicted.cpu().numpy())\n","            all_cnn_labels_test.extend(labels.cpu().numpy())\n","    cnn_final_test_loss = cnn_test_running_loss / len(test_loader)\n","    cnn_final_test_acc = 100 * cnn_test_correct / cnn_test_total\n","    print(f\"CNN Test Loss: {cnn_final_test_loss:.4f}, Test Accuracy: {cnn_final_test_acc:.2f}%\")\n","    plot_confusion_matrix(all_cnn_labels_test, all_cnn_preds_test, classes, model_name='CNN_Test')\n","\n","    print(\"\\n--- Model Comparison (for CNN only) ---\")\n","    print(f\"CNN Final Test Accuracy: {cnn_final_test_acc:.2f}%\")\n","    print(\"\\nDiscussion points (Analyze after running this CNN code):\")\n","    print(\"1. Compare the final test accuracies of MLP and CNN (requires running train_mlp.py separately).\")\n","    print(\"2. Analyze the learning curves (Train vs Validation Loss AND Accuracy) for this CNN model. Look for signs of overfitting or underfitting.\")\n","    print(\"3. Examine the confusion matrix for this CNN model to understand class-wise performance.\")\n","    print(\"4. Discuss why CNN generally performs better than MLP for image classification tasks on CIFAR-10.\")"]}]}